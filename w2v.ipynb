{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527d7230",
   "metadata": {},
   "source": [
    "# Détectez les bad buzz grace au Deeplearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d71684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "# basic libs\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import unidecode\n",
    "import gc\n",
    "from io import StringIO\n",
    "from timeit import default_timer as timer\n",
    "# text preprocessing\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# deeplearning\n",
    "import tensorflow as tf\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "trained = 0\n",
    "\n",
    "done_preprocessing = 1\n",
    "\n",
    "if not done_preprocessing:\n",
    "    trained = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928138de",
   "metadata": {},
   "source": [
    "### Text preproccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0465b",
   "metadata": {},
   "source": [
    "On commence par tokéniser le texte (on ne garde que les mots, en supprimant la pontuation, les liens, les nombres ...).\n",
    "On retire également les \"stop-words\", c'est-à-dire tous les articles, déterminants, pronoms et mots de liaison.\n",
    "\n",
    "On trie le vocabulaire en utilisant comme référence les mots de l'embedding glove entraîné par l'Unversité de Stanford sur un dataset également issu de twitter. Ce traitement permet d'éliminer efficacement les twits pauvres, en se passant du moins possible de mots porteurs de sens.\n",
    "\n",
    "On applique ensuite deux traitements différents séparément pour comparer leurs performances: la lemmatisation (qui garde seulement la forme canonique des mots, par exemple le féminin singulier) et le stemming (qui garde uniquement le radical des mots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be64a1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 20000\n",
    "val_size = 20000\n",
    "\n",
    "max_words = 25\n",
    "\n",
    "NUM_WORDS = 71243"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245a1cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trained or not done_preprocessing:\n",
    "    word_vectors = gensim.models.KeyedVectors.load_word2vec_format('./models/GoogleNews-vectors-negative300.bin', binary=True)\n",
    "    EMBEDDING_DIM=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1de150fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71243 unique tokens in vanilla text\n",
      "Final main dataframe size: 1476873\n"
     ]
    }
   ],
   "source": [
    "def update_target(tgt):\n",
    "    return 1 if tgt else 0\n",
    "\n",
    "def clean_up(text):\n",
    "    text = text.lower()\n",
    "    for exclude in ['&quot;', '&amp;']:\n",
    "        text = text.replace(exclude, ' ')\n",
    "    text = text.replace('-', '')\n",
    "    text = text.split()\n",
    "    for exclude in ['@', '/', 'www']:\n",
    "        text = [w for w in text if not exclude in w]\n",
    "    text = ' '.join(text)\n",
    "    text = unidecode.unidecode(text)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [w for w in tokens if not w in stop_words and w in word_vectors]\n",
    "    if len(tokens) > max_words:\n",
    "        tokens = tokens[:max_words]\n",
    "    return ' '.join(tokens) if len(tokens) > 1 else ''\n",
    "    \n",
    "if not done_preprocessing:\n",
    "    train_df = pd.read_csv('./data/dataset.csv', names=['target', 'id', 'date', 'flag', 'user', 'text'], encoding='latin-1')\n",
    "    train_df = train_df[['target', 'text']]\n",
    "    \n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    stop_words = list(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    train_df[\"text\"] = train_df[\"text\"].apply(clean_up)\n",
    "    \n",
    "    train_df = train_df[train_df['text'] != \"\"]\n",
    "    \n",
    "    vanilla_vectoriser = CountVectorizer()\n",
    "    vanilla_vectoriser.fit(train_df[\"text\"].to_list())\n",
    "    joblib.dump(vanilla_vectoriser, \"./models/w2v_vectoriser.joblib\")\n",
    "    \n",
    "    train_df[\"target\"] = train_df[\"target\"].apply(update_target)\n",
    "    \n",
    "    train_df = train_df.sample(frac=1)\n",
    "    \n",
    "    test_df = train_df.head(test_size)\n",
    "    train_df = train_df.tail(train_df.shape[0] - test_size)\n",
    "    val_df = train_df.head(val_size)\n",
    "    train_df = train_df.tail(train_df.shape[0] - val_size)\n",
    "    test_df.to_csv('./data/w2v_text_test.csv', index=False)\n",
    "    val_df.to_csv('./data/w2v_text_val.csv', index=False)\n",
    "    train_df.to_csv('./data/w2v_text_train.csv', index=False)\n",
    "    \n",
    "    # summary\n",
    "    \n",
    "    print(f\"Found {len(vanilla_vectoriser.vocabulary_)} unique tokens in vanilla text\")\n",
    "    print(f\"Final main dataframe size: {train_df.shape[0]}\")\n",
    "else:\n",
    "    print(\"Found 71243 unique tokens in vanilla text\\nFinal main dataframe size: 1476873\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbdad6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_vocabulary():\n",
    "    with open(\"./azure/w2v_vocabulary.txt\", 'w+') as f:\n",
    "        vanilla_vectoriser = joblib.load(\"./models/w2v_vectoriser.joblib\")\n",
    "        for w in vanilla_vectoriser.vocabulary_:\n",
    "            f.write(w)\n",
    "            f.write('\\n')\n",
    "\n",
    "if not done_preprocessing:            \n",
    "    save_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0eb36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>laying dark thinking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>got watch thts sooooo sweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>outside working garden office</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>one worse days ive wanna go home sleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>watching enough pretty intense haha</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                    text\n",
       "0  1       laying dark thinking                  \n",
       "1  1       got watch thts sooooo sweet           \n",
       "2  1       outside working garden office         \n",
       "3  0       one worse days ive wanna go home sleep\n",
       "4  1       watching enough pretty intense haha   "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(f'./data/w2v_text_train.csv')\n",
    "test_df = pd.read_csv(f'./data/w2v_text_test.csv')\n",
    "val_df = pd.read_csv(f'./data/w2v_text_val.csv')\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417eec36",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = f'./results.csv'\n",
    "\n",
    "if not os.path.isfile(results_file):\n",
    "    results = pd.DataFrame(columns=['model_name', 'accuracy', 'time', 'loss'])\n",
    "    results.to_csv(results_file, index=False)\n",
    "\n",
    "def add_result(name, accuracy, time, loss=None):\n",
    "    result_df = pd.read_csv(results_file)\n",
    "    result_df = result_df[result_df[\"model_name\"] != name]\n",
    "    result_df = result_df.append({\"model_name\": name, \"accuracy\": accuracy, \"time\": time, \"loss\": loss}, ignore_index=True)\n",
    "    result_df.to_csv(results_file, index=False)\n",
    "    \n",
    "def get_results():\n",
    "    result_df = pd.read_csv(results_file)\n",
    "    print(result_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4818c5c",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd29ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "def process_model(model, hist, name, test_dataset, time):\n",
    "    # get results\n",
    "    history = pd.DataFrame(data=hist.history)\n",
    "    model_performance = model.evaluate(test_dataset)\n",
    "    model_pred = model.predict(test_dataset)\n",
    "    # save results\n",
    "    model.save(f\"./models/{name}.h5\")\n",
    "    history.to_csv(f'./models/{name}_history.csv', index=False)\n",
    "    model.save_weights(f'./models/{name}_weights.h5')\n",
    "    add_result(name, model_performance[1], time, model_performance[0])\n",
    "\n",
    "\n",
    "def display_learning_curves(hst):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, met in enumerate(['accuracy', 'loss']):\n",
    "        ax[i].plot(hst[met])\n",
    "        ax[i].plot(hst['val_' + met])\n",
    "        ax[i].set_title('Model {}'.format(met))\n",
    "        ax[i].set_xlabel('epochs')\n",
    "        ax[i].set_ylabel(met)\n",
    "        ax[i].legend(['train', 'val'])\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ddf0d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "data_passes = 2\n",
    "\n",
    "steps_per_epoch = (train_df.shape[0] * data_passes) // (batch_size * n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af9f0ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get targets\n",
    "y_train = np.asarray(train_df[\"target\"].to_list())\n",
    "y_test = np.asarray(test_df[\"target\"].to_list())\n",
    "y_val = np.asarray(val_df[\"target\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7aed0b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequences encoded on 25 words\n"
     ]
    }
   ],
   "source": [
    "if not trained:\n",
    "    X_train = train_df[\"text\"].to_list()\n",
    "    X_test = test_df[\"text\"].to_list()\n",
    "    X_val = val_df[\"text\"].to_list()\n",
    "\n",
    "    def encoder(data_train, data_test, data_val):\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "        tokenizer.fit_on_texts(data_train)\n",
    "        encoded_train = tokenizer.texts_to_sequences(data_train)\n",
    "        encoded_test = tokenizer.texts_to_sequences(data_test)\n",
    "        encoded_val = tokenizer.texts_to_sequences(data_val)\n",
    "        max_len = max([len(s.split()) for s in data_train])\n",
    "        train_data = np.asarray(tf.keras.preprocessing.sequence.pad_sequences(encoded_train, maxlen=max_len, padding='post'))\n",
    "        test_data = np.asarray(tf.keras.preprocessing.sequence.pad_sequences(encoded_test, maxlen=max_len, padding='post'))\n",
    "        val_data = np.asarray(tf.keras.preprocessing.sequence.pad_sequences(encoded_val, maxlen=max_len, padding='post'))\n",
    "\n",
    "        return train_data, test_data, val_data, max_len, tokenizer\n",
    "\n",
    "    X_train, X_test, X_val, vanilla_input_dim, vanilla_tokenizer = encoder(X_train, X_test, X_val)\n",
    "    \n",
    "    joblib.dump(vanilla_tokenizer, \"./azure/w2v_tokenizer.joblib\")\n",
    "      \n",
    "    # make tf datasets\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, tf.cast(y_train, tf.int32))).shuffle(BUFFER_SIZE)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, tf.cast(y_test, tf.int32)))\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, tf.cast(y_val, tf.int32))).shuffle(BUFFER_SIZE)\n",
    "    # preprocess datasets\n",
    "    train_ds = train_ds.repeat()\n",
    "    train_ds = train_ds.batch(batch_size)\n",
    "    train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    test_ds = test_ds.batch(batch_size)\n",
    "    val_ds = val_ds.batch(batch_size)\n",
    "    val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    print(f\"Sequences encoded on {vanilla_input_dim} words\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e316dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not trained:\n",
    "    word_index = vanilla_tokenizer.word_index\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        if i>=NUM_WORDS:\n",
    "            continue\n",
    "        try:\n",
    "            embedding_vector = word_vectors[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            embedding_matrix[i]=np.random.normal(0,np.sqrt(0.25),EMBEDDING_DIM)\n",
    "\n",
    "    embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
    "                                EMBEDDING_DIM,\n",
    "                                weights=[embedding_matrix],\n",
    "                                trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "83bcbc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 300)         21233100  \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, None, 128)         219648    \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 21,461,069\n",
      "Trainable params: 21,461,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/20\n",
      "1153/1153 - 33s - loss: 0.4943 - accuracy: 0.7574 - val_loss: 0.4733 - val_accuracy: 0.7736\n",
      "Epoch 2/20\n",
      "1153/1153 - 30s - loss: 0.4659 - accuracy: 0.7777 - val_loss: 0.4603 - val_accuracy: 0.7815\n",
      "Epoch 3/20\n",
      "1153/1153 - 30s - loss: 0.4574 - accuracy: 0.7827 - val_loss: 0.4554 - val_accuracy: 0.7837\n",
      "Epoch 4/20\n",
      "1153/1153 - 30s - loss: 0.4546 - accuracy: 0.7839 - val_loss: 0.4526 - val_accuracy: 0.7882\n",
      "Epoch 5/20\n",
      "1153/1153 - 30s - loss: 0.4548 - accuracy: 0.7846 - val_loss: 0.4496 - val_accuracy: 0.7886\n",
      "Epoch 6/20\n",
      "1153/1153 - 30s - loss: 0.4512 - accuracy: 0.7877 - val_loss: 0.4507 - val_accuracy: 0.7865\n",
      "Epoch 7/20\n",
      "1153/1153 - 30s - loss: 0.4446 - accuracy: 0.7908 - val_loss: 0.4467 - val_accuracy: 0.7893\n",
      "Epoch 8/20\n",
      "1153/1153 - 30s - loss: 0.4463 - accuracy: 0.7896 - val_loss: 0.4437 - val_accuracy: 0.7932\n",
      "Epoch 9/20\n",
      "1153/1153 - 30s - loss: 0.4401 - accuracy: 0.7933 - val_loss: 0.4418 - val_accuracy: 0.7936\n",
      "Epoch 10/20\n",
      "1153/1153 - 30s - loss: 0.4404 - accuracy: 0.7928 - val_loss: 0.4393 - val_accuracy: 0.7951\n",
      "Epoch 11/20\n",
      "1153/1153 - 30s - loss: 0.4289 - accuracy: 0.8000 - val_loss: 0.4408 - val_accuracy: 0.7930\n",
      "Epoch 12/20\n",
      "1153/1153 - 30s - loss: 0.4212 - accuracy: 0.8050 - val_loss: 0.4432 - val_accuracy: 0.7911\n",
      "Epoch 13/20\n",
      "1153/1153 - 30s - loss: 0.4159 - accuracy: 0.8084 - val_loss: 0.4399 - val_accuracy: 0.7941\n",
      "Epoch 14/20\n",
      "1153/1153 - 30s - loss: 0.4170 - accuracy: 0.8066 - val_loss: 0.4413 - val_accuracy: 0.7942\n",
      "Epoch 15/20\n",
      "1153/1153 - 30s - loss: 0.4171 - accuracy: 0.8074 - val_loss: 0.4417 - val_accuracy: 0.7936\n",
      "Epoch 16/20\n",
      "1153/1153 - 30s - loss: 0.4155 - accuracy: 0.8088 - val_loss: 0.4429 - val_accuracy: 0.7939\n",
      "Epoch 17/20\n",
      "1153/1153 - 30s - loss: 0.4083 - accuracy: 0.8137 - val_loss: 0.4396 - val_accuracy: 0.7929\n",
      "Epoch 18/20\n",
      "1153/1153 - 30s - loss: 0.4118 - accuracy: 0.8102 - val_loss: 0.4425 - val_accuracy: 0.7916\n",
      "Epoch 19/20\n",
      "1153/1153 - 30s - loss: 0.4062 - accuracy: 0.8132 - val_loss: 0.4405 - val_accuracy: 0.7947\n",
      "Epoch 20/20\n",
      "1153/1153 - 30s - loss: 0.4067 - accuracy: 0.8132 - val_loss: 0.4396 - val_accuracy: 0.7947\n",
      "157/157 [==============================] - 1s 4ms/step - loss: 0.4342 - accuracy: 0.7969\n"
     ]
    }
   ],
   "source": [
    "model_name = \"w2v_lstm_nn\"\n",
    "\n",
    "def train_w2v_lstm_model():\n",
    "    \n",
    "    w2v_model = tf.keras.models.Sequential()\n",
    "    w2v_model.add(embedding_layer) \n",
    "    w2v_model.add(tf.keras.layers.LSTM(128,return_sequences=True,dropout=0.2))\n",
    "    w2v_model.add(tf.keras.layers.GlobalMaxPooling1D())\n",
    "    w2v_model.add(tf.keras.layers.Dense(64,activation='relu')) \n",
    "    w2v_model.add(tf.keras.layers.Dense(1,activation='sigmoid')) \n",
    "\n",
    "    print(w2v_model.summary())\n",
    "    \n",
    "    w2v_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    t0 = timer()\n",
    "    hist = w2v_model.fit(train_ds,\n",
    "                         epochs=n_epochs,\n",
    "                         verbose=2,\n",
    "                         validation_data=val_ds,\n",
    "                         batch_size=batch_size,\n",
    "                         steps_per_epoch=steps_per_epoch)\n",
    "    t1 = timer() - t0\n",
    "    process_model(w2v_model, hist, model_name, test_ds, t1)\n",
    "\n",
    "\n",
    "if not trained:\n",
    "    train_w2v_lstm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52e65f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "if trained==1:\n",
    "    history = pd.read_csv(f'./models/{model_name}_history.csv')\n",
    "    display_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc91d80",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d2b151a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            model_name  accuracy         time      loss\n",
      "0   random_forest       0.74330   4673.442075 NaN      \n",
      "1   random_forest_stem  0.74460   3145.852605 NaN      \n",
      "2   random_forest_lem   0.74370   4217.279929 NaN      \n",
      "3   vanilla_nn          0.76795   4371.061097  0.484366\n",
      "4   stemming_nn         0.76415   3354.503793  0.491544\n",
      "5   lemmatization_nn    0.76780   3995.933181  0.485960\n",
      "6   glove_cnn           0.78720   652.948430   0.451748\n",
      "7   glove_lstm_nn       0.79640   772.612858   0.444426\n",
      "8   glove_lstm_cnn      0.78160   814.337397   0.482108\n",
      "9   ASTA                0.72600  NaN          NaN      \n",
      "10  w2v_lstm_nn         0.79690   611.238391   0.434156\n"
     ]
    }
   ],
   "source": [
    "get_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
