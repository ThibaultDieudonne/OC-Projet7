{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "527d7230",
   "metadata": {},
   "source": [
    "# Détectez les bad buzz grace au Deeplearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d71684a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# import tensorflow as tf\n",
    "import nltk\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import gensim\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import joblib\n",
    "\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.width', None)\n",
    "# pd.set_option('display.max_colwidth', -1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928138de",
   "metadata": {},
   "source": [
    "### Text preproccessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd0465b",
   "metadata": {},
   "source": [
    "On commence par tokéniser le texte (on ne garde que les mots, en supprimant la pontuation, les liens, les nombres ...).\n",
    "On retire également les \"stop-words\", c'est-à-dire tous les articles, déterminants, pronoms et mots de liaison.\n",
    "On applique ensuite deux traitements différents séparément pour comparer leurs performances: la lemmatisation (qui garde seulement la forme canonique des mots, par exemple le féminin singulier) et le stemming (qui garde uniquement le radical des mots). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de150fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_preprocessing = 1\n",
    "\n",
    "n_words = 5000 # warning: must be higher later\n",
    "\n",
    "test_size = 100000\n",
    "val_size = 100000\n",
    "\n",
    "lbls = ['Negative', '', '', '', 'Positive']\n",
    "\n",
    "if not done_preprocessing:\n",
    "    train_df = pd.read_csv('./data/dataset.csv', names=['target', 'id', 'date', 'flag', 'user', 'text'], encoding='latin-1')\n",
    "    train_df = train_df[['target', 'text']]\n",
    "    \n",
    "    tokenizer = nltk.RegexpTokenizer(r'\\w+')\n",
    "    stop_words = list(nltk.corpus.stopwords.words('english'))\n",
    "    \n",
    "    def clean_up(text):\n",
    "        text = ''.join(c for c in text if not c.isdigit()).replace('\\n', '').lower()\n",
    "        text = text.split()\n",
    "        for exclude in ['@', '/']:\n",
    "            text = [w for w in text if not exclude in w]\n",
    "        text = ' '.join(text)\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [w for w in tokens if not w in stop_words]\n",
    "        return \" \".join(tokens)\n",
    "    \n",
    "    train_df[\"text\"] = train_df[\"text\"].apply(clean_up)\n",
    "    \n",
    "    train_df = train_df[train_df['text'] != \"\"]\n",
    "    \n",
    "    train_df.to_csv('./data/preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0530a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 225037 unique tokens after stemming\n",
      "Found 259292 unique tokens after lemmatisation\n",
      "Final dataframe size: 1560604\n"
     ]
    }
   ],
   "source": [
    "if not done_preprocessing:\n",
    "    \n",
    "    train_df = pd.read_csv('./data/preprocessed.csv')\n",
    "    \n",
    "    stemmer = EnglishStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def stem(text):\n",
    "        return [stemmer.stem(w) for w in text.split()]\n",
    "\n",
    "    def lem(text):\n",
    "        return [lemmatizer.lemmatize(w) for w in text.split()]\n",
    "\n",
    "    # stemming\n",
    "    train_df[\"text_stem\"] = train_df[\"text\"].apply(stem)\n",
    "    s_dictionary = gensim.corpora.Dictionary(train_df[\"text_stem\"].tolist())\n",
    "    print(f\"Found {len(s_dictionary)} unique tokens after stemming\")\n",
    "    s_dictionary.filter_extremes(no_below=1000, no_above=0.4, keep_n=n_words)\n",
    "    s_words = s_dictionary.token2id\n",
    "    \n",
    "    # lemmatization\n",
    "    train_df[\"text_lem\"] = train_df[\"text\"].apply(lem)\n",
    "    l_dictionary = gensim.corpora.Dictionary(train_df[\"text_lem\"].tolist())\n",
    "    print(f\"Found {len(l_dictionary)} unique tokens after lemmatisation\")\n",
    "    l_dictionary.filter_extremes(no_below=1000, no_above=0.4, keep_n=n_words)\n",
    "    l_words = l_dictionary.token2id\n",
    "    \n",
    "    def clean_stem(tokens):\n",
    "        return \" \".join([t for t in tokens if t in s_words])\n",
    "        \n",
    "    def clean_lem(tokens):\n",
    "        return \" \".join([t for t in tokens if t in l_words])\n",
    "\n",
    "    train_df[\"text_stem\"] = train_df[\"text_stem\"].apply(clean_stem)\n",
    "    train_df[\"text_lem\"] = train_df[\"text_lem\"].apply(clean_lem)\n",
    "\n",
    "    train_df = train_df[train_df['text_stem'] != \"\"]\n",
    "    train_df = train_df[train_df['text_lem'] != \"\"]\n",
    "    \n",
    "    train_df = train_df.sample(frac=1)\n",
    "    \n",
    "    print(f\"Final dataframe size: {train_df.shape[0]}\")\n",
    "    \n",
    "    test_df = train_df.head(test_size)\n",
    "    train_df = train_df.tail(train_df.shape[0] - test_size)\n",
    "    val_df = train_df.head(val_size)\n",
    "    train_df = train_df.tail(train_df.shape[0] - val_size)\n",
    "    test_df.to_csv('./data/text_test.csv', index=False)\n",
    "    val_df.to_csv('./data/text_val.csv', index=False)\n",
    "    train_df.to_csv('./data/text_train.csv', index=False)\n",
    "else:\n",
    "    print(\"Found 225037 unique tokens after stemming\\nFound 259292 unique tokens after lemmatisation\\nFinal dataframe size: 1560604\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb0eb36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>text_stem</th>\n",
       "      <th>text_lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>best day watched brother kick top post footy m...</td>\n",
       "      <td>best day watch brother kick top post miss p</td>\n",
       "      <td>best day watched brother kick top post miss p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>fall back asleep</td>\n",
       "      <td>fall back asleep</td>\n",
       "      <td>fall back asleep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>heading work</td>\n",
       "      <td>head work</td>\n",
       "      <td>heading work</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>wow wish made bad girls club longer fucking mi...</td>\n",
       "      <td>wow wish made bad girl club longer fuck miss</td>\n",
       "      <td>wow wish made bad girl club longer fucking miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>dunt wud prefere watch mark perform anyday pms...</td>\n",
       "      <td>prefer watch mark perform u play nite</td>\n",
       "      <td>watch mark u playing nite</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target                                               text  \\\n",
       "0       4  best day watched brother kick top post footy m...   \n",
       "1       0                                   fall back asleep   \n",
       "2       4                                       heading work   \n",
       "3       0  wow wish made bad girls club longer fucking mi...   \n",
       "4       4  dunt wud prefere watch mark perform anyday pms...   \n",
       "\n",
       "                                      text_stem  \\\n",
       "0   best day watch brother kick top post miss p   \n",
       "1                              fall back asleep   \n",
       "2                                     head work   \n",
       "3  wow wish made bad girl club longer fuck miss   \n",
       "4         prefer watch mark perform u play nite   \n",
       "\n",
       "                                          text_lem  \n",
       "0    best day watched brother kick top post miss p  \n",
       "1                                 fall back asleep  \n",
       "2                                     heading work  \n",
       "3  wow wish made bad girl club longer fucking miss  \n",
       "4                        watch mark u playing nite  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if done_preprocessing:\n",
    "    train_df = pd.read_csv('./data/text_train.csv')\n",
    "    test_df = pd.read_csv('./data/text_test.csv')\n",
    "    val_df = pd.read_csv('./data/text_val.csv')\n",
    "    \n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4818c5c",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Le modèle de base servant de références aux autres modèles est une simple régression (random forest) appliquée sur les bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84ec123",
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = 1\n",
    "\n",
    "debug = 1\n",
    "\n",
    "if debug:\n",
    "    train_df = train_df.head(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0d74548",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectoriser = CountVectorizer(max_features=n_words)\n",
    "train_vectorized_stem = vectoriser.fit_transform(train_df[\"text_stem\"].to_list()).toarray()\n",
    "test_vectorized_stem = vectoriser.fit_transform(test_df[\"text_stem\"].to_list()).toarray()\n",
    "train_vectorized_lem = vectoriser.fit_transform(train_df[\"text_lem\"].to_list()).toarray()\n",
    "test_vectorized_lem = vectoriser.fit_transform(test_df[\"text_lem\"].to_list()).toarray()\n",
    "target = train_df[\"target\"].to_list()\n",
    "test_target = test_df[\"target\"].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0fdd68",
   "metadata": {},
   "source": [
    "#### With stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db2f030",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debug:\n",
    "    rfc=RandomForestClassifier(random_state=42)\n",
    "    param_grid = { \n",
    "        'n_estimators': [100, 1000],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    forest_grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "    forest_grid.fit(train_vectorized_stem, target)\n",
    "    print(forest_grid.best_params_)\n",
    "    print(\"accuracy :\", forest_grid.best_score_)\n",
    "    forest_params = forest_grid.best_params_\n",
    "else:\n",
    "    forest_params = {'max_features': 'sqrt', 'n_estimators': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56945d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "if trained:\n",
    "    random_forest_stem = joblib.load(\"./random_forest_stem.joblib\")\n",
    "else:\n",
    "    random_forest_stem = RandomForestClassifier(\n",
    "                    n_estimators=forest_params['n_estimators'],\n",
    "                    random_state=50,\n",
    "                    max_features=forest_params['max_features'],\n",
    "                    verbose=False,\n",
    "                    n_jobs=-1)\n",
    "    random_forest_stem.fit(train_vectorized_stem, target)\n",
    "    joblib.dump(random_forest_stem, \"./random_forest_stem.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c3ef745",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.71336"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_forest_stem.score(test_vectorized_stem, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99ed51d",
   "metadata": {},
   "source": [
    "#### With lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "112b1b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debug:\n",
    "    rfc=RandomForestClassifier(random_state=42)\n",
    "    param_grid = { \n",
    "        'n_estimators': [100, 1000],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    forest_grid = GridSearchCV(estimator=rfc, param_grid=param_grid, cv=10, scoring=\"accuracy\", n_jobs=-1)\n",
    "    forest_grid.fit(train_vectorized_lem, target)\n",
    "    print(forest_grid.best_params_)\n",
    "    print(\"accuracy :\", forest_grid.best_score_)\n",
    "    forest_params = forest_grid.best_params_\n",
    "else:\n",
    "    forest_params = {'max_features': 'sqrt', 'n_estimators': 1000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74d93c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7108"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if trained:\n",
    "    random_forest_lem = joblib.load(\"./random_forest_lem.joblib\")\n",
    "else:\n",
    "    random_forest_lem = RandomForestClassifier(\n",
    "                    n_estimators=forest_params['n_estimators'],\n",
    "                    random_state=50,\n",
    "                    max_features=forest_params['max_features'],\n",
    "                    verbose=False,\n",
    "                    n_jobs=-1)\n",
    "    random_forest_lem.fit(train_vectorized_lem, target)\n",
    "    joblib.dump(random_forest_lem, \"./random_forest_lem.joblib\")\n",
    "    \n",
    "random_forest_lem.score(test_vectorized_lem, test_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
